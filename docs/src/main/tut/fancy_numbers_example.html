<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>fancy_numbers_example - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":false,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":true,"spark_heap_memory":20396,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":44632,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":93104,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":190048,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":237560,"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":8079,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10099,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":19998,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge","support_cluster_tags":true,"container_memory_mb":24998,"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":43837,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge","support_cluster_tags":true,"container_memory_mb":54796,"memory_mb":61440,"category":"Compute Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":20396,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"memory_mb":31232,"category":"Storage Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":44632,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"memory_mb":62464,"category":"Storage Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":93104,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"memory_mb":124928,"category":"Storage Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":190048,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"memory_mb":249856,"category":"Storage Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":44632,"instance_type_id":"p2.xlarge","node_type_id":"p2.xlarge","description":"p2.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":55790,"memory_mb":62464,"category":"GPU Accelerated","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":383936,"instance_type_id":"p2.8xlarge","node_type_id":"p2.8xlarge","description":"p2.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":479920,"memory_mb":499712,"category":"GPU Accelerated","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":577824,"instance_type_id":"p2.16xlarge","node_type_id":"p2.16xlarge","description":"p2.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":722280,"memory_mb":749568,"category":"GPU Accelerated","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":false,"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized","support_cluster_tags":false,"container_memory_mb":28000,"memory_mb":30720,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":false},{"support_ssh":false,"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized","support_cluster_tags":false,"container_memory_mb":12128,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":false}],"default_node_type_id":"r3.xlarge"},"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-east-1c","isDefault":true},{"id":"us-east-1e","isDefault":false},{"id":"us-east-1d","isDefault":false},{"id":"us-east-1b","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":true,"enableElasticSparkUI":true,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":true,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-6fb640835bd45a2e2095758663e237aefe80671acacc2e6377eec5ecccb9004b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-ca4b827492f83a5a1402629a8aa9e58d1ea1f0f6b1f81b1f1c8fffc7bab62ce9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-22ee201c31a037446476259c6efb835d5da16d8e49a677a6c532065503c14e0a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":true,"enableClusterAutoScaling":true,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":true,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.37.271","accountsLimit":-1,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":30,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"support@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"17f8674cecf9882f9b4c91e061c62541d11e2449","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":true,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"accounts":true,"useOnDemandClustersByDefault":false,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":210729,"name":"fancy_numbers_example","language":"scala","commands":[{"version":"CommandV1","origId":211209,"guid":"4d20e2b1-7803-467f-a827-4d27b9427554","subtype":"command","commandType":"auto","position":0.5,"command":"%md #A Fancy (Numbers) Example\n\nThis notebook demonstrates the use of [Spark Records](https://swoop-inc.github.io/spark-records/) in a scenario where we face the typical problems of big data in the real world: invalid inputs, complex business rules and code quality issues.\n\nThe material builds upon [Spark Records by example](https://swoop-inc.github.io/spark-records/docs.html#spark-records-by-example) and dives deeper into the capabilities of the Spark Records framework: <br> \n\n- Ignoring some input data based on business rules\n\n- Custom metric collection and data quality checks\n\n- Capturing additional information about the data in the records' row-level logs\n\n- Known error identification\n\n- Root cause analysis in the presence of both known and unknown errors\n\n- Error mitigation strategies\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745142E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c433fe87-ede6-4e69-a17c-eeea4b3548fd"},{"version":"CommandV1","origId":211210,"guid":"46f22c6e-9778-4f95-a6c7-8072bcb6891e","subtype":"command","commandType":"auto","position":0.75,"command":"%md ## Running this notebook\n\nIf you want to run this notebook you have to set up your environment. Skip this section if you prefer to treat the notebook as documentation.\n\n1. Get a [Databricks](https://www.databricks.com) account. If you are not a Databricks customer, you can create a free [Databricks Community Edition](https://community.cloud.databricks.com/) account.\n\n2. Create a new cluster. (`Clusters` / `Create Cluster`) Note that Spark Records requires Spark version 2.x or higher and Scala 2.11.   \n**Tip:** We strongly recommend NOT running Spark on Scala 2.10 because the reflection API, which Spark extensively uses, is not thread-safe. Occasionally, this leads to very strange behavior.\n\n3. Get a copy of the Spark Records JAR. You can build it yourself or grab the latest released version from [https://bintray.com/swoop-inc/maven/spark-records](https://bintray.com/swoop-inc/maven/spark-records).\n\n4. Import the Spark Records library. (`Workspace` / `Create` /  `Library`) Add the Spark Records JAR and attach it to your cluster (or choose to have it automatically attached to all clusters).\n\n5. Import this notebook by using `Import Notebook` from this page or `Workspace` / `Import` and then providing the notebook's URL ([https://swoop-inc.github.io/spark-records/fancy_numbers_example.html](https://swoop-inc.github.io/spark-records/fancy_numbers_example.html)).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745191E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3b4db11d-e4ed-4a58-9a4b-ec10f7f04d98"},{"version":"CommandV1","origId":211213,"guid":"c25d8707-3cce-4ee9-a61f-360aaca974cb","subtype":"command","commandType":"auto","position":0.875,"command":"%md ## The fancy_numbers example\n\nThe test suite for Spark Records includes an example in the [`examples.fancy_numbers`](https://github.com/swoop-inc/spark-records/blob/master/src/test/scala/examples/fancy_numbers) package whose goal is to categorize numbers. Because the test classes are not included in the Spark Records distribution, the following cell contains the entire code of [`Example.scala`](https://github.com/swoop-inc/spark-records/blob/master/src/test/scala/examples/fancy_numbers/Example.scala).\n\nNote that we are using a [package cell](https://docs.cloud.databricks.com/docs/latest/databricks_guide/01%20Databricks%20Overview/13%20Advanced%20Features/00%20Package%20Cells%20in%20Notebooks.html) to make sure the classes are defined in the `examples.fancy_numbers` package, exactly as they would be in the Spark Records test suite.\n\nFollow the comments in the code.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745249E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4b93a00e-d686-43d8-b3fc-332aed7cb8c7"},{"version":"CommandV1","origId":210731,"guid":"4b5a5989-5b14-47b9-9348-a46da8480469","subtype":"command","commandType":"auto","position":1.0,"command":"package examples.fancy_numbers\n\nimport com.swoop.spark.records._\n\n/** Let's create a simple but not simplistic example of data processing with Spark.\n  *\n  * We have to categorize integers in the range [0, 100] in the following way:\n  *\n  * - Numbers outside of [0, 100] are errors.\n  *\n  * - 0s should be ignored.\n  *\n  * - Prime numbers are categorized as `prime`.\n  *\n  * - Perfect numbers, those that are equal to the sums of their factors, e.g.,\n  * 6 = 3 + 2 + 1, are categorized as `perfect`.\n  *\n  * - 13 is categorized as `bakers-dozen`\n  *\n  * - Everything else is categorized as `even` or `odd`.\n  *\n  * This example is simple because the inputs are integers but it is not simplistic\n  * because we have to deal with errors and inputs that should not produce any output.\n  * Further, just to make things a little more interesting, we'll introduce a bug in our code.\n  *\n  * Here is how we'd like to represent our output data.\n  *\n  * @param n        the input integer\n  * @param category the integer's category\n  */\ncase class FancyNumber(n: Int, category: String)\n\n\n/** Wrapping our data with a record is simple: we extend `Record[OurData, OurInput]` and\n  * then provide defaults. Below you see the minimum set of fields you need in a record.\n  * You can add more fields if you need more information in the wrapper. Common examples include\n  * adding a timestamp, a record ID or schema versioning information.\n  *\n  * @see [[com.swoop.spark.records.Record]]\n  * @see [[com.swoop.spark.records.Issue]]\n  * @param features A bit mask that allows records to be easily filtered\n  * @param data     The data the record wraps. May be empty in the case of some error records.\n  * @param source   The source of the data. This is how we keep track of data provenance.\n  *                 A common optimization is to only include the source for error records.\n  * @param flight   Several jobs that work together to produce a result share the same flight ID.\n  *                 How you choose to organize this is up to you. The ID is often a UUIDv4.\n  * @param issues   The [[issues]] field contains the record-level \"log file\".\n  */\ncase class FancyNumberRecord(\n  features: Int,\n  data: Option[FancyNumber] = None,\n  source: Option[Int] = None,\n  flight: Option[String] = None,\n  issues: Option[Seq[Issue]] = None\n) extends Record[FancyNumber, Int]\n\n\n/** This is just a holder for our example code */\nobject Example {\n\n  /** A record builder for a simple use case like the one we are dealing with should\n    * extend [[RecordBuilder]].\n    *\n    * [[JobContext]] provides state management and other services during building.\n    * In this example, we'll use it collect some custom statistics.\n    *\n    * @see [[RecordBuilder]]\n    * @see [[JobContext]]\n    * @see [[BuildContext]]\n    * @param n  the input integer to categorize\n    * @param jc the job context\n    */\n  case class Builder(n: Int, override val jc: JobContext)\n    extends RecordBuilder[Int, FancyNumber, FancyNumberRecord, JobContext](n, jc) {\n\n    /** This is the main method we have to implement. Here we build an\n      * [[examples.fancy_numbers.FancyNumber]] from the input.\n      *\n      * @return the record data generated for this input or `None`\n      */\n    def buildData: Option[FancyNumber] = {\n      // Use throwError to throw _identifiable exceptions_, ones you can easily find\n      // after the job has finished executing\n      if (n < 0 || n > 100) throwError(\"input outside [0, 100]\", n.toString, Err.OUT_OF_RANGE)\n\n      // jc.inc() is how we can collect custom metrics during job execution\n      val stat = if (n % 2 == 0) \"even\" else \"odd\"\n      jc.inc(s\"numbers.$stat\")\n\n      if (n == 0) None // returning None will skip this input and generate no record\n      else Some(FancyNumber(n, category))\n    }\n\n    /** Returns a record with valid data.\n      * This method is a hook to allow you to create your own custom records.\n      * It uses the state that has been accumulated during the building of the record's data,\n      * namely, `features` and `issues`.\n      *\n      * When everything is going well, we save on storage by not saving the source of the data.\n      *\n      * @param data   the data produced in [[buildData]]\n      * @param issues the issues collected during the building process\n      * @return A valid record with data\n      */\n    def dataRecord(data: FancyNumber, issues: Seq[Issue]): FancyNumberRecord =\n      FancyNumberRecord(features, Some(data), None, jc.flight, issues)\n\n    /** Returns an error record.\n      * This method is called if the record will contain an error.\n      * \n      * Depending on whether the error occurred during or after [[buildData]], the resulting\n      * record may or may not have `data`. In our case `maybeData` will always be `None` \n      * because there is no code path where an error is generated after [[buildData]].\n      *\n      * @param issues the issues collected during the building process, at least one of\n      *               which will be an error\n      * @return A valid error record\n      */\n    def errorRecord(issues: Seq[Issue]): FancyNumberRecord =\n      FancyNumberRecord(features, maybeData, Some(n), jc.flight, issues)\n\n    /** Returns the category of the input number. */\n    def category: String =\n      if (primesTo100(n)) \"prime\"\n      else if (n == 6 || n == 28) {\n        // warn(), info() and debug() let you store messages inside the `issues` collection\n        // associated with each record. Debug messages are particularly useful during\n        // development. Features.QUALITY_CONCERN is one of the predefined record flags.\n        warn(\"rare number\", \"perfect, really?\", Features.QUALITY_CONCERN)\n        \"perfect\"\n      }\n      else if (n == 13) \"bakers-dozen\"\n      else if (n % 2 == 0) \"even\"\n      else \"odd\"\n\n  }\n\n  /** One of the key principles of bulletproof big data processing is controlling failure modes\n    * and ensuring that errors can be quickly separated in appropriate buckets, e.g., known vs.\n    * unknown, safely ignorable vs. something one has to act on, etc.\n    *\n    * In Spark records, the first step in doing this involves associating IDs with known/expected\n    * error types.\n    */\n  object Err {\n    /** The ID of the error raised when we get an out-of-range number. */\n    val OUT_OF_RANGE = 1001\n  }\n\n  /** Integers IDs are great for big data processing but not great for comprehension.\n    * Fear not: you can associated descriptive messages with error (and warning, info, debug)\n    * IDs in the following manner. The Spark Records root cause analysis framework will\n    * automatically make these available without adding them to the data.\n    */\n  Issue.addMessagesFor(Issue.Error, Map(\n    Err.OUT_OF_RANGE -> \"number out of range\"\n  ))\n\n  /** This little bit of API sugar makes it easier to plug a builder into Spark's RDD and\n    * Dataset APIs. It is not required but makes code simpler & cleaner.\n    */\n  def buildRecords[Ctx <: JobContext](inputs: TraversableOnce[Int], jc: Ctx)\n  : Iterator[FancyNumberRecord] =\n    inputs.flatMap(Builder(_, jc).build).toIterator\n\n  /** A simple prime number algorithm that builds a boolean array telling us whether a\n    * number is prime. We've introduced an off-by-one error in the implementation on\n    * purpose to show how Spark Records deals with unexpected failures (as opposed to\n    * out of range input, which is an expected failure).\n    *\n    * @see http://alvinalexander.com/text/prime-number-algorithm-scala-scala-stream-class\n    */\n  private lazy val primesTo100 = {\n    def primeStream(s: Stream[Int]): Stream[Int] = Stream.cons(s.head, primeStream(s.tail filter {\n      _ % s.head != 0\n    }))\n\n    val arr = Array.fill(100)(false) // bug: should be 101 because of 0-based indexing\n    primeStream(Stream.from(2)).takeWhile(_ <= 100).foreach(arr(_) = true)\n    arr\n  }\n\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486322737164E12,"submitTime":1.486322745304E12,"finishTime":1.4863227375E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"acea56d3-9ff5-4e43-9c35-6ce452a65c37"},{"version":"CommandV1","origId":211214,"guid":"0d412174-5661-478c-9c54-4988f045a13c","subtype":"command","commandType":"auto","position":1.25,"command":"%md ## Defining the Spark job\n\nWe'll represent our job as a simple function that, given some numbers and a table name, builds a dataset of `FancyNumberRecords` and saves is into the table. Immediately following this, we perform a data quality check requiring that Spark Records saw the number of inputs we provided and that the error rate is less than 1%.\n\nNote that we are passing in the driver context. This way, if there was an exception during job execution, we could query the driver context for accumulated statistics, etc.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745342E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"de0e4ad4-9972-43ff-b8d7-53152066af7f"},{"version":"CommandV1","origId":210737,"guid":"2197ba7a-c4f9-4e96-875f-43be03778dd3","subtype":"command","commandType":"auto","position":1.5,"command":"import com.swoop.spark.records._\nimport examples.fancy_numbers._ \n\ndef saveRecords(dc: SimpleDriverContext, numbers: Seq[Int], tableName: String) = {\n  val jc = dc.jobContext(SimpleJobContext)\n\n  spark.createDataset(numbers)\n    .mapPartitions(inputs => Example.buildRecords(inputs, jc))\n    .write.mode(SaveMode.Overwrite).saveAsTable(tableName)\n  \n  jc.checkDataQuality(minInputs=numbers.length, maxErrorRate=0.01, maxSkippedRate=0.001)\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import com.swoop.spark.records._\nimport examples.fancy_numbers._\nsaveRecords: (dc: com.swoop.spark.records.SimpleDriverContext, numbers: Seq[Int], tableName: String)Unit\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486322737504E12,"submitTime":1.4863227454E12,"finishTime":1.48632273791E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"726fb110-5796-490e-a282-637502661061"},{"version":"CommandV1","origId":211216,"guid":"6e75d371-6f6c-4c65-8dd2-b7ffac9492d4","subtype":"command","commandType":"auto","position":1.625,"command":"%md ## Running the job\n\nWe create the driver context in a separate cell for the same reason we pass it into `saveRecords`: in order to have it accessible in the case of an exception during cell execution.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745436E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2e248eee-1e0a-45d9-badb-40160e9c9968"},{"version":"CommandV1","origId":210753,"guid":"ccacf051-6106-49ef-89af-4cdb672f5989","subtype":"command","commandType":"auto","position":1.75,"command":"val tableName = \"fancy_numbers_neg5to100\"\nval dc = SimpleDriverContext(sc)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">tableName: String = fancy_numbers_neg5to100\ndc: com.swoop.spark.records.SimpleDriverContext = com.swoop.spark.records.SimpleDriverContext@5b2802cb\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486322737914E12,"submitTime":1.4863227455E12,"finishTime":1.486322738203E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3c300ae0-0135-40cc-9275-2d95d599abaf"},{"version":"CommandV1","origId":211217,"guid":"8226f68d-daf7-4aa5-abc9-b2a09e477cbb","subtype":"command","commandType":"auto","position":1.875,"command":"%md It's time to run the job.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745533E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"20710077-ba29-4a33-ad95-8bfc4b071d8e"},{"version":"CommandV1","origId":210735,"guid":"177024f5-fdd7-420f-9e26-06fb5d818c83","subtype":"command","commandType":"auto","position":2.0,"command":"saveRecords(dc, -5 to 100, tableName)","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"E00004: Too many errors. (inputs: 106, skipped: 1, errors: 6, withData: 99)","error":"<div class=\"ansiout\">\tat com.swoop.spark.records.SimpleJobContext$.err$1(JobContext.scala:39)\n\tat com.swoop.spark.records.SimpleJobContext$.checkDataQuality(JobContext.scala:46)\n\tat com.swoop.spark.records.SimpleJobContext.checkDataQuality(JobContext.scala:14)\n\tat line26165bb8f2224a8b96d7658f3941aff767.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.saveRecords(&lt;console&gt;:64)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:62)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:69)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:71)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:73)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:75)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:77)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:79)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:81)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:83)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:85)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:87)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:89)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:91)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:93)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:95)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:97)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:99)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:101)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw$$iw.&lt;init&gt;(&lt;console&gt;:103)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$read$$iw.&lt;init&gt;(&lt;console&gt;:105)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$eval$.$print$lzycompute(&lt;console&gt;:7)\n\tat line26165bb8f2224a8b96d7658f3941aff771.$eval$.$print(&lt;console&gt;:6)</div>","workflows":[],"startTime":1.486322745587E12,"submitTime":1.486322745587E12,"finishTime":1.486322751687E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"60936f08-4015-44c6-b6ed-bd5a5c3943bb"},{"version":"CommandV1","origId":211220,"guid":"20be28e9-82ca-4297-944b-7f603683ec1c","subtype":"command","commandType":"auto","position":2.25,"command":"%md Our data quality check failed: there were too many errors, far above the `maxErrorRate` of 1% we specified. (The error rate is `#errors/#withData`, which is roughly 6%).\n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745622E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4227d94c-cf39-4d66-a9c9-9b474ba5b310"},{"version":"CommandV1","origId":211221,"guid":"8119b63c-0c5b-4076-a954-d91103b05573","subtype":"command","commandType":"auto","position":2.375,"command":"%md ## Root cause analysis\n\nA good first step in root cause analysis is looking at the metrics collected during job execution.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745701E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fd133a35-7421-4e01-b458-9c030ae52925"},{"version":"CommandV1","origId":210751,"guid":"04e139b8-1a36-444b-af6d-4f144efc627e","subtype":"command","commandType":"auto","position":2.5,"command":"dc.printStats","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">input.count: 106\nissue.category.1: 6\nissue.category.2: 2\nissue.count: 8\nissue.id.1-1001: 5\nissue.id.2-1024: 2\nnumbers.even: 51\nnumbers.odd: 50\nrecord.count: 105\nrecord.data.count: 99\nrecord.empty.count: 1\nrecord.error.count: 6\nrecord.features.0: 97\nrecord.features.1: 6\nrecord.features.2: 2\nrecord.skipped.count: 1\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.486322747486E12,"submitTime":1.486322757046E12,"finishTime":1.486322747699E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"61b50ea7-0eb7-4a1d-86ce-4af30eed58df"},{"version":"CommandV1","origId":211222,"guid":"4a64f775-19b2-475b-b6c4-9ccc3695581a","subtype":"command","commandType":"auto","position":3.75,"command":"%md Apart from `numbers.even` and `numbers.odd`, all other metrics were automatically collected by Spark Records. \n\nTo an experienced user of Spark Records, a few things will immediately jump out in the output. There are 6 error issues (`issue.category.1: 6` and [`Issue.Error.categoryId == 1`](https://github.com/swoop-inc/spark-records/blob/master/src/main/scala/com/swoop/spark/records/Issue.scala)) but only 5 of the errors are ones we've previously encountered (`issue.id.1-1001: 5` or 5 issues were errors with issue ID 1001, which is `Example.Err.OUT_OF_RANGE`).\n\nLuckily, we don't have to remember category and issue IDs to figure this out. Spark Records provides many tools that make root cause analysis very fast.\n\nLet's start by loading the job output and caching it to speed up queries.\n\n**Tip:** If you are dealing with a lot of data, caching the entire dataset may not be feasible. However, in most cases, caching `records.errorRecords` would work equally well when you are drilling into unknown errors.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745794E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e0a5f61c-2198-4108-bf8f-f8fa7f799cfb"},{"version":"CommandV1","origId":210740,"guid":"2feff75b-8b1f-4722-9263-b5e7eee9e923","subtype":"command","commandType":"auto","position":5.0,"command":"val records = spark.table(\"fancy_numbers_neg5to100\").as[FancyNumberRecord].cache","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">records: org.apache.spark.sql.Dataset[examples.fancy_numbers.FancyNumberRecord] = [features: int, data: struct&lt;n: int, category: string&gt; ... 3 more fields]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.486322752836E12,"submitTime":1.486322762402E12,"finishTime":1.486322753711E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"47adbb07-bcf6-46b4-9bb3-347c07f8476d"},{"version":"CommandV1","origId":211231,"guid":"1bce6e91-2a02-43f1-9fcc-463a767d2174","subtype":"command","commandType":"auto","position":5.75,"command":"%md Before using the root cause analysis tooling, we have to teach Spark Records how to efficiently separate error records from data records. Since we have not partitioned our data, we are in a flat record environment.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322745907E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a6569d09-5c6b-40ff-b4da-6391fb81f425"},{"version":"CommandV1","origId":210744,"guid":"bd39adaa-8130-475c-8621-cbb87ce4f88a","subtype":"command","commandType":"auto","position":6.5,"command":"implicit val recordEnv = new FlatRecordEnvironment()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">recordEnv: com.swoop.spark.records.FlatRecordEnvironment = com.swoop.spark.records.FlatRecordEnvironment@5343ed5d\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.486322755648E12,"submitTime":1.486322765207E12,"finishTime":1.486322755917E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"42f1bbb2-43e1-4a04-9718-10590c840b48"},{"version":"CommandV1","origId":211232,"guid":"45bc2a0a-3e50-4645-976e-fc5e5e52c08f","subtype":"command","commandType":"auto","position":6.625,"command":"%md A good first step is getting an overview of all issues in the job output.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322746002E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0e61c9c3-f20d-4e92-98ca-a11eb2cd19cc"},{"version":"CommandV1","origId":210799,"guid":"77c1248c-1e20-4ed3-8dcb-0f5edb82a42a","subtype":"command","commandType":"auto","position":6.75,"command":"display(records.issueCounts)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["E","number out of range",5.0,["E01001: input outside [0, 100]"]],["E",null,1.0,["java.lang.ArrayIndexOutOfBoundsException: 100"]],["W",null,2.0,["W01024: rare number"]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"category_type","type":"\"string\"","metadata":"{}"},{"name":"id_message","type":"\"string\"","metadata":"{}"},{"name":"cnt","type":"\"long\"","metadata":"{}"},{"name":"messages","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.486322760578E12,"submitTime":1.48632277016E12,"finishTime":1.486322761995E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"1753","height":"159","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ae3cfe9e-797f-4b43-bc75-a2c720244e47"},{"version":"CommandV1","origId":211233,"guid":"1992bf73-36a4-40ef-bfcb-5c8e0b9fcfaf","subtype":"command","commandType":"auto","position":6.8125,"command":"%md The framework tools automatically converted category and issue IDs to meaningful text.\n\nWe see 5 out of range errors (-5 to -1 in our input). There is one unknown error (`id_message` is `null`). We also see two warnings for the \"rare\" [perfect numbers](https://en.wikipedia.org/wiki/Perfect_number) (6 and 28).\n\n**Tip:** In a real rush we could focus and look at error issues only.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322746099E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ff0ad8e8-d2a4-475a-a443-357a38561465"},{"version":"CommandV1","origId":211234,"guid":"e4855815-4403-408b-a019-c3d51d7b2e45","subtype":"command","commandType":"auto","position":6.84375,"command":"display(records.errorIssueCounts)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["E","number out of range",5.0,["E01001: input outside [0, 100]"]],["E",null,1.0,["java.lang.ArrayIndexOutOfBoundsException: 100"]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"category_type","type":"\"string\"","metadata":"{}"},{"name":"id_message","type":"\"string\"","metadata":"{}"},{"name":"cnt","type":"\"long\"","metadata":"{}"},{"name":"messages","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.486322762317E12,"submitTime":1.486322771877E12,"finishTime":1.486322763565E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"1750","height":"125","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"132d8b0c-2c95-4503-ac12-9f90a752f7ae"},{"version":"CommandV1","origId":211235,"guid":"192617db-9c59-4cf5-a56b-98acbb14e1b1","subtype":"command","commandType":"auto","position":6.859375,"command":"%md Let's find out more about the unexpected error.\n\nBecause exceptions sometimes originate deep inside framework code, we are specifically interested in the line in our code, which may or may not be the first line in the exception stack trace, that caused the exception. To find the answer, we'll ask Spark Records for unknown error details focused in our code, which is from the `examples.fancy_numbers` package.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322746204E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5bfb4d52-9789-4ad2-907f-e3345b01c9b3"},{"version":"CommandV1","origId":210800,"guid":"534f7f9a-db9a-4463-a22c-8c62e4651757","subtype":"command","commandType":"auto","position":6.875,"command":"display(records.unknownErrorDetails(\"examples.fancy_numbers\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["java.lang.ArrayIndexOutOfBoundsException: 100",["examples.fancy_numbers.Example$Builder","category","<driver>",123.0],[1.0,100.0,"edc4baaf-8545-45aa-a907-d1fadff8dec8",null,[[1.0,"java.lang.ArrayIndexOutOfBoundsException: 100",[["100",[["examples.fancy_numbers.Example$Builder","category","<driver>",123.0],["examples.fancy_numbers.Example$Builder","buildData","<driver>",92.0],["com.swoop.spark.records.RecordBuilder","captureData","RecordBuilder.scala",213.0],["com.swoop.spark.records.RecordBuilder$$anonfun$build$3","apply","RecordBuilder.scala",199.0],["com.swoop.spark.records.RecordBuilder$$anonfun$build$3","apply","RecordBuilder.scala",199.0],["scala.util.Try$","apply","Try.scala",192.0],["com.swoop.spark.records.RecordBuilder","build","RecordBuilder.scala",199.0],["examples.fancy_numbers.Example$$anonfun$buildRecords$1","apply","<driver>",163.0],["examples.fancy_numbers.Example$$anonfun$buildRecords$1","apply","<driver>",163.0]]]],null,null]]]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"message","type":"\"string\"","metadata":"{}"},{"name":"location","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"className\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"methodName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fileName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lineNumber\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"sample_record","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"features\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"source\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flight\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"data\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"n\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"category\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"issues\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"category\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"message\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"causes\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"message\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stack\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"className\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"methodName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fileName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lineNumber\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"details\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.486322763936E12,"submitTime":1.48632277352E12,"finishTime":1.48632276546E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"1807","height":"304","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"378946be-da38-412a-b588-9b3eabbfbe32"},{"version":"CommandV1","origId":211236,"guid":"4329b0a2-d808-478a-812f-f0fc41983498","subtype":"command","commandType":"auto","position":7.375,"command":"%md There you have it. The exception originated from line 123 in the `category()` method of `Example.Builder`. From the sample record, we know that it happened while we were processing the input `100`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322746338E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"35673b66-dc25-49bb-9134-36822f6f1b79"},{"version":"CommandV1","origId":211136,"guid":"ce54711e-29a7-4288-922d-494e9fcecdd6","subtype":"command","commandType":"auto","position":7.875,"command":"%md ## Sidebar: the benefits of the Spark Records approach\n\n\"Wait a moment.\", you might say. \"The exception message `java.lang.ArrayIndexOutOfBoundsException: 100` already tells me that the problem very likely happened when the input was 100 and there is only one place in my code where I index into a array. Why do I need Spark Records?\"\n\nConsider the following: <br>\n\n- Without Spark Records, the job would have immediately failed on the first input (-5), hiding from you the following key pieces of information:\n\n    - The frequency of out-of-range errors in the data, which matters when it comes to choosing a mitigation strategy.\n\n    - The existence of the off-by-one bug in the code. You'd only discover that error after deciding on a mitigation strategy for out-of-range errors, changing the code, re-deploying and re-running the job. Remember the days--if you are of a certain age, that is--when compilers used to fail immediately on the first error in any one file? Remember how many times you had to recompile to discover all compilation problems? There is a huge benefit in knowing about all errors up front. You don't want to have to rerun a job many times to find out all the possible ways it can fail.\n\n- Often, when incoming data is dirty, there will be many errors and log files will run into the tens of gigabytes. Without a simple way to group errors by incidence count and partition them into known vs. unknown it could take a long time to understand and prioritize what needs fixing.\n\n- The meaning of `java.lang.ArrayIndexOutOfBoundsException: 100` is obvious in our simple example with a limited set of known, simple inputs. When the input data is large or complex, e.g., JSON documents with dozens or hundreds of fields in a complex structure, exception messages often make very little sense without the source dat, especially when they originate in framework code that is called from many places in the application code.\n\n- Further, most common ways to group errors, even in commercial packages, focus either just on the origination point of the exception or on the entire stack trace. The former typically groups disparate errors together and the latter provides no easy one-line summary to grok what's happening. `None.get` or `NullPointerException` are not helpful as summaries. When exception causes are nested, the applicaton code may not even be in the stack trace of the topmost exception. That's the reason why Spark Records let's you customize the behavior of root cause analysis down to writing a custom filtering expression for the stack trace elements that should be considered for display in the `location` column above.\n\n###Advanced root cause analysis\n\nBecause all the error information, including nested causes is part of a Spark dataset, you can develop your own custom tools for advanced root cause analysis. For example, at [Swoop](https://www.swoop.com) we have root cause analysis queries that use `get_json_object()` to peek inside the source JSON data.\n\nThe next notebook demonstrates [advanced root cause analysis](https://swoop-inc.github.io/spark-records/advanced_root_cause_analysis.html).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322746413E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"14058840-7a79-412c-90c6-52c4432a322e"},{"version":"CommandV1","origId":211238,"guid":"f3de44a1-9a0b-4d13-9873-65b472181428","subtype":"command","commandType":"auto","position":9.875,"command":"%md ##Mitigation\n\nNow that all the causes of the job failure are known, we should consider how we'd like to deal with them. To simplify the analysis, let's assume we have no control over the upstream system feeding numbers to our job.\n\nIf we expect to occasionally see invalid inputs from upstream producers, there are two typical mitigation strategies. We could still generate errors for invalid inputs and build a custom data quality check for the job that allows a higher rate of invalid input errors. Alternatively, we could skip invalid inputs, use a custom metric to track them and only fail jobs if the rate of invalid inputs seems unusually high. The difference between the two approaches is in whether you have access to the data that caused a job failure. In the first case, you would, but you pay for this with having to store all the invalid inputs. You could combine the two strategies by only skipping a fraction of the invalid records. This way you'd have a sample of invalid records to look at if a job fails. In advanced scenarios there is a hybrid option. It uses custom partitioning to separate certain types of invalid records. The partitions with that data are deleted some time after the job completes. You'd have to be dealing with huge amounts of data with a high natural invalid input rate to make the extra operational and code complexity of this approach worth the effort.\n\nAs far as the unexpected index of out bounds error is concerned, we'll just fix the off-by-one error in the code. :-)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322746486E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dd89a375-fdd8-4dd4-8501-481e04770e16"},{"version":"CommandV1","origId":211239,"guid":"0e549a15-a0ac-48b9-bf79-ec057ffd8862","subtype":"command","commandType":"auto","position":10.875,"command":"%md Back to [Spark Records](https://swoop-inc.github.io/spark-records/index.html).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486322746545E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d31d1a21-5dee-461f-83f5-722c0c6b0120"}],"dashboards":[],"guid":"77923ad7-a2bc-4e27-8622-f2481e2ea1f0","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
