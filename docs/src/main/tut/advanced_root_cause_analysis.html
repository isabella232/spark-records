<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>advanced_root_cause_analysis - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":false,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":true,"spark_heap_memory":20396,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":44632,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":93104,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":190048,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":237560,"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":8079,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10099,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":19998,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge","support_cluster_tags":true,"container_memory_mb":24998,"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":43837,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge","support_cluster_tags":true,"container_memory_mb":54796,"memory_mb":61440,"category":"Compute Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":20396,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"memory_mb":31232,"category":"Storage Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":44632,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"memory_mb":62464,"category":"Storage Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":93104,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"memory_mb":124928,"category":"Storage Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":190048,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"memory_mb":249856,"category":"Storage Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":44632,"instance_type_id":"p2.xlarge","node_type_id":"p2.xlarge","description":"p2.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":55790,"memory_mb":62464,"category":"GPU Accelerated","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":383936,"instance_type_id":"p2.8xlarge","node_type_id":"p2.8xlarge","description":"p2.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":479920,"memory_mb":499712,"category":"GPU Accelerated","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"spark_heap_memory":577824,"instance_type_id":"p2.16xlarge","node_type_id":"p2.16xlarge","description":"p2.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":722280,"memory_mb":749568,"category":"GPU Accelerated","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":false,"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized","support_cluster_tags":false,"container_memory_mb":28000,"memory_mb":30720,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":false},{"support_ssh":false,"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized","support_cluster_tags":false,"container_memory_mb":12128,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":false}],"default_node_type_id":"r3.xlarge"},"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-east-1c","isDefault":true},{"id":"us-east-1e","isDefault":false},{"id":"us-east-1d","isDefault":false},{"id":"us-east-1b","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":true,"enableElasticSparkUI":true,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":true,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-6fb640835bd45a2e2095758663e237aefe80671acacc2e6377eec5ecccb9004b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-ca4b827492f83a5a1402629a8aa9e58d1ea1f0f6b1f81b1f1c8fffc7bab62ce9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-22ee201c31a037446476259c6efb835d5da16d8e49a677a6c532065503c14e0a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":true,"enableClusterAutoScaling":true,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":true,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.37.271","accountsLimit":-1,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":30,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"support@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"17f8674cecf9882f9b4c91e061c62541d11e2449","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":true,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"accounts":true,"useOnDemandClustersByDefault":false,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":211069,"name":"advanced_root_cause_analysis","language":"scala","commands":[{"version":"CommandV1","origId":211103,"guid":"0dfa0a2a-554b-4324-bb83-d2a237262872","subtype":"command","commandType":"auto","position":0.1875,"command":"%md #Advanced Root Cause Analysis\n\nBy following the patterns of [Spark Records](https://swoop-inc.github.io/spark-records/) you gain the benefits lightning-fast root cause analysis even if your records were built using tooling different from the one provided by the framework. \n\nThis notebook demonstrates how this can be done using Python and SparkSQL. We will analyze the output of the `FancyNumbers` job from [this notebook](https://swoop-inc.github.io/spark-records/fancy_numbers_example.html) in three steps:  <br>  \n\n\n1. Prepare job output for analysis.\n\n2. Generate issue summaries.\n\n3. Drill down to find the exact location of unknown errors.\n\nBeyond showing how to do root cause analysis directly against records, this notebook also demonstrates how to customize the analysis to your particular job & input/output data.\n\n**Tip:** At [Swoop](https://www.swoop.com) we reuse common root cause analysis operations by putting them in a shared notebook that we simply `%include` into the notebook we are currently working in. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330791968E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b2fb26dd-0a35-4e33-aa66-b9abd729a157"},{"version":"CommandV1","origId":211104,"guid":"d346ce5d-bc53-4216-a7bf-fb96c12d1f56","subtype":"command","commandType":"auto","position":0.28125,"command":"%md ##Prepare job output for analysis\n\nWe grab the data we need + map it to a standard temporary view name: `records_to_investigate` so that we don't have to change our SparkSQL every time we want to look at a new dataset. \n\nWe also print the schema of the records because it helps us write queries faster. Note that every record may have 0 or more `issues`, each of which can have 0 or more `causes`, each of which has a `stack` of many source locations. This is how Spark Records captures extremely detailed exception information with nested causes and full stack traces.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792033E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8406fb65-07c0-42f0-9b78-798bd29c5b6a"},{"version":"CommandV1","origId":211089,"guid":"37e9f418-7b38-4d06-8471-163b4a204d8a","subtype":"command","commandType":"auto","position":0.375,"command":"%python \nspark.table(\"fancy_numbers_neg5to100\").createOrReplaceTempView(\"records_to_investigate\")\nspark.table(\"records_to_investigate\").printSchema()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- features: integer (nullable = true)\n |-- data: struct (nullable = true)\n |    |-- n: integer (nullable = true)\n |    |-- category: string (nullable = true)\n |-- source: integer (nullable = true)\n |-- flight: string (nullable = true)\n |-- issues: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- category: integer (nullable = true)\n |    |    |-- message: string (nullable = true)\n |    |    |-- causes: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- message: string (nullable = true)\n |    |    |    |    |-- stack: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- className: string (nullable = true)\n |    |    |    |    |    |    |-- methodName: string (nullable = true)\n |    |    |    |    |    |    |-- fileName: string (nullable = true)\n |    |    |    |    |    |    |-- lineNumber: integer (nullable = true)\n |    |    |-- id: integer (nullable = true)\n |    |    |-- details: string (nullable = true)\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486330783352E12,"submitTime":1.486330792101E12,"finishTime":1.486330783476E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3de188fd-8b1a-48c1-87e0-dcb2a87e734c"},{"version":"CommandV1","origId":211107,"guid":"49a01ba0-1287-46c0-93fb-cf2bf8364c2c","subtype":"command","commandType":"auto","position":0.9375,"command":"%md ## Generate an issue summary\n\nA good first step is to get an overview summary of all the issues in the saved data. We want to see:  \n<br>\n\n\n- `category`: the issue category\n- `id`: the issue ID\n- `cnt`: a count of how many issues of this type we've seen\n- `messages`: a summary of all distinct messages associated with this issue ID (there could be more than one in some cases)\n- `sources`: a summary of all distinct inputs associated with the issue. \n\n**Tip:** When we write custom root cause analysis code we can really tune it for quick comprehension. Our source summaries depend on the fact that we are dealing with a small input set and that the inputs themselves are small (just integers). In a complex real world example, it's better to just include a sample source (using `first()` instead of `collect_set()`).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792138E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2a77f658-3ef9-46db-ac3f-4f387c6e84a7"},{"version":"CommandV1","origId":211083,"guid":"e3b994a7-077c-4098-8998-fa438f0dc3f4","subtype":"command","commandType":"auto","position":1.5,"command":"%sql\nselect \n  issue.category,\n  issue.id,\n  count(*)                              as cnt,\n  collect_set(issue.message)            as messages,\n  collect_set(coalesce(source, data.n)) as sources -- we only save sources for errors but we know the number is also in data.n\nfrom records_to_investigate\nlateral view explode(issues) exploded as issue\nwhere features <> 0 -- we filter to records that have any feature flag set\ngroup by issue.category, issue.id\norder by issue.category, issue.id","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0,null,1.0,["java.lang.ArrayIndexOutOfBoundsException: 100"],[100.0]],[1.0,1001.0,5.0,["E01001: input outside [0, 100]"],[-3.0,-2.0,-5.0,-1.0,-4.0]],[2.0,1024.0,2.0,["W01024: rare number"],[6.0,28.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"category","type":"\"integer\"","metadata":"{}"},{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"cnt","type":"\"long\"","metadata":"{}"},{"name":"messages","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"sources","type":"{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.48633078348E12,"submitTime":1.486330792201E12,"finishTime":1.486330784201E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"49db6d7b-eb05-42af-b4c1-de9f1cdfde9c"},{"version":"CommandV1","origId":211108,"guid":"c39fbd13-76bf-4225-9b27-f094b270aec0","subtype":"command","commandType":"auto","position":1.875,"command":"%md We have the output we expect. It shows two types of errors (5 for out of range numbers `-5..-1`, one for the index out of bounds exception caused by the off-by-one bug in the code) and two warnings for the perfect numbers. \n\nHowever, there is something unsatisfying about the output: we have to decode the meaning of `category` and `id` from their numeric values, which is not great. With a bit of work we can do much better.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792244E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c52b66b8-942e-463e-a602-b57d01598f5d"},{"version":"CommandV1","origId":211109,"guid":"aee46445-f462-4bef-a5e1-ccecc7b5f9fe","subtype":"command","commandType":"auto","position":2.0625,"command":"%md ### Issue ID descriptions\n\nBecause the mapping of issue IDs to descriptions changes infrequently, we can build a table with the information and join it against our root cause analysis tables.\n\nWe should not forget to include the default, framework-provided, IDs. We can collect them with one line in Scala or by reading the [source code](https://github.com/swoop-inc/spark-records/blob/master/src/main/scala/com/swoop/spark/records/Issue.scala).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792306E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f996454c-534d-4381-85e5-da6211cdfaa5"},{"version":"CommandV1","origId":211110,"guid":"eb6018ac-256e-4a61-938a-de5ad855d215","subtype":"command","commandType":"auto","position":2.15625,"command":"import com.swoop.spark.records._\n\nSeq(Issue.Error, Issue.Warning, Issue.Info, Issue.Debug).map(o => (o.featureMask, o.idMessages)).foreach(println)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">(1,Map(0 -&gt; unknown, 1 -&gt; internal error, 2 -&gt; unsupported schema, 3 -&gt; missing required argument, 4 -&gt; data quality check failed))\n(2,Map(0 -&gt; unknown))\n(4,Map(0 -&gt; unknown))\n(8,Map(0 -&gt; unknown))\nimport com.swoop.spark.records._\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486330784206E12,"submitTime":1.486330792364E12,"finishTime":1.486330784647E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0e0a60b1-a616-4bc4-9e17-6fe62dc84541"},{"version":"CommandV1","origId":211112,"guid":"8a014353-cf07-4efe-acb5-6ea027102378","subtype":"command","commandType":"auto","position":2.1796875,"command":"%md With a few lines of Python we map this information and our own application specific IDs into a table. Again, we'll create a temp view in order to save on rewriting SQL later.\n\n**Tip:** This can be done ahead of time or as part of a shared \"prologue\" for your root cause analysis toolkit.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792399E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"677a9e6b-addb-44ec-898c-6729f4d98648"},{"version":"CommandV1","origId":211111,"guid":"581c2eae-8c78-4723-8890-695cff1cdcc6","subtype":"command","commandType":"auto","position":2.203125,"command":"%python\n# Dictionary of issue categories mapping to issue IDs mapping to messages associated with these IDs\nid_messages = {\n  1 : { # error\n    0: \"unknown\",\n    1: \"internal error\",\n    2: \"unsupported schema\",\n    3: \"missing required parameter\",\n    4: \"data quality check failed\",\n    # from our fancy numbers example\n    1001: \"negative number\"\n  },\n  2 : { # warning\n    0: \"unknown\"\n  },\n  4 : { # info\n    0: \"unknown\"\n  },\n  8 : { # debug\n    0: \"unknown\"\n  }\n}\n\n# Flatten the above into an array of dictionaries\nlist = []\nfor category, messages in id_messages.iteritems():\n  for issue_id, id_message in messages.iteritems():\n    list.append({\"category\": category, \"id\": issue_id, \"id_message\": id_message})\n\n# Save the mapping into a table\n# We may get a warning: \"inferring schema from dict is deprecated\". Yes, but it is rather expedient in this case. :-)\nspark.createDataFrame(list).write.mode(\"overwrite\").saveAsTable(\"fancy_numbers_id_messages\")\n\n# Create a temp view with a more standard name so that we can use %sql cells in the notebook\nspark.table(\"fancy_numbers_id_messages\").createOrReplaceTempView(\"records_id_messages\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486330784653E12,"submitTime":1.48633079246E12,"finishTime":1.486330788275E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f7ae8b98-a194-4cc3-8e7b-4283079dcf98"},{"version":"CommandV1","origId":211114,"guid":"b6ed99ff-9173-42d4-bcd3-1072cfaffd82","subtype":"command","commandType":"auto","position":2.21484375,"command":"%md Now that we've ready, let's get more detail on just the errors.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792498E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"55ff54c0-385d-4931-9471-6fa2b4485d8f"},{"version":"CommandV1","origId":211113,"guid":"baf5459a-f6ce-40a7-8144-90f67510a66f","subtype":"command","commandType":"auto","position":2.2265625,"command":"%sql\nwith\nissues as (\n  select \n    issue.category,\n    issue.id,\n    count(*)                   as cnt,\n    collect_set(issue.message) as messages,\n    collect_set(source)        as sources\n  from records_to_investigate\n  lateral view explode(issues) exploded as issue\n  where (features & 1) <> 0\n  group by issue.category, issue.id\n)\nselect\n  lhs.id, id_message, cnt, messages, sources\nfrom issues as lhs\n-- Use a left outer join in order not to remove rows without an entry in records_id_messages\nleft outer join records_id_messages rhs\non lhs.category = rhs.category and lhs.id = rhs.id\norder by cnt desc","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1001.0,"negative number",5.0,["E01001: input outside [0, 100]"],[-3.0,-2.0,-5.0,-1.0,-4.0]],[null,null,1.0,["java.lang.ArrayIndexOutOfBoundsException: 100"],[100.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"id_message","type":"\"string\"","metadata":"{}"},{"name":"cnt","type":"\"long\"","metadata":"{}"},{"name":"messages","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"sources","type":"{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486330788279E12,"submitTime":1.486330792558E12,"finishTime":1.486330789684E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"04201838-8ca5-44ac-8dbc-fef2b3e6f02f"},{"version":"CommandV1","origId":211138,"guid":"d2dc96e7-7af2-4905-b14b-b86bfce086d7","subtype":"command","commandType":"auto","position":2.232421875,"command":"%md We did it. We have the issue descriptions.\n\n**Tip:** You can use the same pattern to join other metadata about known issues. This is particularly useful when you'd like to filter out expected errors, which you would only want to be concerned about if they exceed a certain threshold. For example, at [Swoop](https://www.swoop.com) we work with many thousands of publishers who sometimes integrate us into their websites and mobile apps incorrectly. The data generated from bad integrations is invalid and generates error records. However, because the problem is typically self-correcting, our data quality checks allow for many such errors before an alert is triggered.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792596E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8f79194d-cf68-4f76-8be5-ffa1fb5f970c"},{"version":"CommandV1","origId":211115,"guid":"97b752dc-6d09-462e-8722-792e6672a46b","subtype":"command","commandType":"auto","position":2.23828125,"command":"%md ##Find the source of unexpected errors\n\nThe output above shows us one unexpected error: the index out of bounds exception. We want to find out what's causing it.\n\nBecause exceptions are sometimes thrown from deep within framework code, e.g., the dreaded `null` dereference errors (`NullPointerException` in Java, `None.get` in Scala, `NoneType` errors in Python), it would speed up our root cause analysis if we were to separate the _exception origin_ (where it was raised) from the _exception cause_ in our code. The exception cause in our code is the line that invoked whatever caused the exception. In other words, it is the first element on the stack trace from our codebase. In this particular example, this is any code whose `className` is in the `examples.fancy_numbers` package.\n\nPulling this information out in SparkSQL, without resorting to UDFs is not pretty but is also not very difficult. Keep in mind that, because of the consistency that comes with following the schema of Spark Records, you'd only have to write something of this complexity once and then you can reuse it both across failed runs of the same job and across many other jobs with no tweaks or with just minor tweaks.\n\n**Tip:** If you are dealing with _very_ big data and you have frequent need to perform this type of root cause analysis, you can meaninfully speed the process up by writing a custom UDF that processes the nested arrays of causes and stack elements directly. Here we wanted to stick to SparkSQL because it can be applied in any programming language and environment. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792654E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a0d6d044-ec17-4af1-b8f3-ade1c543752a"},{"version":"CommandV1","origId":211126,"guid":"5b255bf4-f196-451a-93c9-bc6c18fc7d40","subtype":"command","commandType":"auto","position":3.90625,"command":"%sql \nwith \nerror_records (\n  -- Filter records to ones containing error issues only.\n  select *,\n    -- Assign unique row IDs to the records, since we'll need to partition the data when we look for the first stack trace position in our code.\n    -- We wouldn't need this if Spark allowed unsorted windows, e.g., lag(col) over (). \n    -- Normally, unsorted windows don't make sense in distributed systems but they do make sense with rows generated by LATERAL VIEW as there is implicit linear ordering there\n    -- Here we use a random UUIDv4 by invoking a the statis randomUUID() method of java.util.UUID\n    reflect('java.util.UUID','randomUUID') as row_id\n  from records_to_investigate\n  where (features & 1) <> 0 -- records marked as having errors only\n),\nall_locations as (\n  -- Explode issues, causes and stack trace elements.\n  -- Keep only the first element of a stack track or all the elements from our code (class names from the examples.fancy_numbers package)\n  select *,\n    -- Find the stack trace position of the previous stack element.\n    -- The result will be null for the first stack element and 0 for the first stack element in our code after the first stack element\n    lag(stack_pos) over (\n      partition by row_id, issue_pos, cause_pos \n      order by stack_pos)                                      as prev_stack_pos,\n    -- Find the class name of the previous stack element.\n    -- As with prev_stack_pos, the result will be null for the first stack element\n    lag(stack.className) over (\n      partition by row_id, issue_pos, cause_pos \n      order by stack_pos)                                      as prev_class_name\n  from error_records\n  lateral view posexplode(issues) exploded_issues              as issue_pos, issue\n  lateral view posexplode(issue.causes) exploded_causes        as cause_pos, cause\n  lateral view posexplode(cause.stack) exploded_stack_elements as stack_pos, stack\n  where issue.category = 1  -- error issues\n    and issue.id is null    -- unknown ones\n    and (    stack_pos = 0  -- first stack element\n          or stack.className regexp 'examples\\.fancy_numbers\\.') -- or a stack element from our code\n),\nselected_locations as (\n  -- Filter locations to just the exception origin or the first location in our code.\n  -- Handle the special case where our code may be first in the stack trace.\n  select *,\n    if(prev_stack_pos is null, stack, null)                             as exception_origin,\n    if(stack.className regexp 'examples\\.fancy_numbers\\.', stack, null) as our_location\n  from all_locations\n  where (prev_stack_pos is null or prev_stack_pos = 0)\n    and (prev_class_name is null or (not prev_class_name regexp 'examples\\.fancy_numbers\\.'))\n)\n-- Summarize the results.\n-- We group by the first stack trace location in our code.\n-- We summarize the exception origins (there could be more than one if the line in our code did multiple things) \nselect \n  count(distinct row_id)        as cnt_records,          -- number of records where the exception is at this location in our code\n  first(our_location, true)     as our_location,         -- first stack trace element in our code\n  collect_set(issue.message)    as messages,             -- summary of exception messages at this location\n  collect_set(exception_origin) as exception_locations,  -- unique exception origins\n  collect_set(source)           as sources,              -- summary of unique inputs causing the exception\n  first(named_struct(\n    'features', features,\n    'source', source,\n    'flight', flight,\n    'data', data,\n    'issues', issues\n  ))                            as sample_record         -- record sample\nfrom selected_locations\ngroup by our_location\nhaving our_location is not null\norder by cnt_records desc","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0,["examples.fancy_numbers.Example$Builder","category","<driver>",123.0],["java.lang.ArrayIndexOutOfBoundsException: 100"],[["examples.fancy_numbers.Example$Builder","category","<driver>",123.0]],[100.0],[1.0,100.0,"edc4baaf-8545-45aa-a907-d1fadff8dec8",null,[[1.0,"java.lang.ArrayIndexOutOfBoundsException: 100",[["100",[["examples.fancy_numbers.Example$Builder","category","<driver>",123.0],["examples.fancy_numbers.Example$Builder","buildData","<driver>",92.0],["com.swoop.spark.records.RecordBuilder","captureData","RecordBuilder.scala",213.0],["com.swoop.spark.records.RecordBuilder$$anonfun$build$3","apply","RecordBuilder.scala",199.0],["com.swoop.spark.records.RecordBuilder$$anonfun$build$3","apply","RecordBuilder.scala",199.0],["scala.util.Try$","apply","Try.scala",192.0],["com.swoop.spark.records.RecordBuilder","build","RecordBuilder.scala",199.0],["examples.fancy_numbers.Example$$anonfun$buildRecords$1","apply","<driver>",163.0],["examples.fancy_numbers.Example$$anonfun$buildRecords$1","apply","<driver>",163.0]]]],null,null]]]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"cnt_records","type":"\"long\"","metadata":"{}"},{"name":"our_location","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"className\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"methodName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fileName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lineNumber\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"messages","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"exception_locations","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"className\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"methodName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fileName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lineNumber\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"},{"name":"sources","type":"{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}","metadata":"{}"},{"name":"sample_record","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"features\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"source\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flight\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"data\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"n\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"category\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"issues\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"category\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"message\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"causes\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"message\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stack\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"className\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"methodName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fileName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lineNumber\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"details\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486330789688E12,"submitTime":1.486330792715E12,"finishTime":1.486330792276E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6372e1c3-3172-4338-88ef-7812108ffcc7"},{"version":"CommandV1","origId":211131,"guid":"b5138a8b-a0fc-41c7-b47f-ac557b990e15","subtype":"command","commandType":"auto","position":4.734375,"command":"%md There you have it. The exception was thrown from line 123 in method `category`. The actual line in the code is `if (primesTo100(n)) \"prime\"`: where we index into the primes array that's too short by one element.\n\nNote that the filename in the `our_location` column is `<driver>`. That's the problem with tracing errors to code entered in REPLs or notebooks, which are pretty frontends to REPLs. Until notebooks start showing line numbers, put large amounts of code codes in libraries. This will make root cause analysis much faster.\n\nIn this case, our code is the origin of the exception, which is why `exception_locations` shows only one entry that's the same as `our_location`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792756E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e77aad81-6f25-473b-93ba-3dbf1ed2fd79"},{"version":"CommandV1","origId":211248,"guid":"7f7d40a2-485b-4b27-ac81-75d752649e76","subtype":"command","commandType":"auto","position":5.251953125,"command":"%md Back to [Spark Records](index.html).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.486330792822E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6051817a-f6a8-43f1-aa3a-3dbc96fb7778"}],"dashboards":[],"guid":"3c0bd497-ad4f-4420-b6ca-21d53b8211c8","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
